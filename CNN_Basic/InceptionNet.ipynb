{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InceptionNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTsVipmKOEAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7907514d-cba3-4da2-e60c-bb529095afdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 17 07:21:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "ppmQ12ePjqSw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InceptionNet"
      ],
      "metadata": {
        "id": "VxVYvQ9Xj5DN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inception Block"
      ],
      "metadata": {
        "id": "REETU1Hwj7ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, c1, c2, c3, c4):\n",
        "        '''\n",
        "        Input\n",
        "        c1(int): number of filters for 1 x 1 convolution layer of path 1\n",
        "        c2(tuple): number of filters for 1 x 1 convolution layer and 3 x 3 convolution layer of path 2\n",
        "        c3(tuple): number of filters for 1 x 1 convolution layer and 5 x 4 convolution layer of path 3\n",
        "        c4(int): number of filters for 1 x 1 convolution layer of path 4\n",
        "        '''\n",
        "        super(InceptionBlock, self).__init__()\n",
        "\n",
        "        self.p1_1 = tf.keras.layers.Conv2D(filters=c1, kernel_size=1,\n",
        "                                           padding='same', activation='relu')\n",
        "        \n",
        "        self.p2_1 = tf.keras.layers.Conv2D(filters=c2[0], kernel_size=1,\n",
        "                                           activation='relu')\n",
        "        self.p2_2 = tf.keras.layers.Conv2D(filters=c2[1], kernel_size=3,\n",
        "                                           padding='same', activation='relu')\n",
        "        \n",
        "        self.p3_1 = tf.keras.layers.Conv2D(filters=c3[0], kernel_size=1,\n",
        "                                           activation='relu')\n",
        "        self.p3_2 = tf.keras.layers.Conv2D(filters=c3[1], kernel_size=5,\n",
        "                                           padding='same', activation='relu')\n",
        "        \n",
        "        self.p4_1 = tf.keras.layers.MaxPool2D(pool_size=3, strides=1, padding='same')\n",
        "        self.p4_2 = tf.keras.layers.Conv2D(c4, 1, activation='relu')\n",
        "\n",
        "    def call(self, x):\n",
        "        p1 = self.p1_1(x)\n",
        "        p2 = self.p2_2(self.p2_1(x))\n",
        "        p3 = self.p3_2(self.p3_1(x))\n",
        "        p4 = self.p4_2(self.p4_1(x))\n",
        "\n",
        "        return tf.keras.layers.Concatenate()([p1, p2, p3, p4])\n"
      ],
      "metadata": {
        "id": "Ka-fpX3PjtjB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.random.uniform((1, 28, 28, 192))\n",
        "block = InceptionBlock(64, (96, 128), (16, 32), 32)\n",
        "block(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0X5EVzykwzg",
        "outputId": "d8ac3604-3b90-43d5-8282-3d8d2915bba2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 28, 28, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## InceptionNet(GooGLENet)"
      ],
      "metadata": {
        "id": "92PuqWLPzN92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionNet(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        '''\n",
        "        Input\n",
        "        num_classes(int): \n",
        "        '''\n",
        "        super(InceptionNet, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=7,\n",
        "                                            strides=2, padding='same',\n",
        "                                            activation='relu')\n",
        "        self.maxpool1 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2,\n",
        "                                                  padding='same')\n",
        "        self.conv2 = tf.keras.layers.Conv2D(filters=192, kernel_size=3, \n",
        "                                            strides=1, padding='same',\n",
        "                                            activation='relu')\n",
        "        self.maxpool2 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2,\n",
        "                                                  padding='same')\n",
        "        \n",
        "        self.inception3a = InceptionBlock(64, (96, 128), (16, 32), 32)\n",
        "        self.inception3b = InceptionBlock(128, (128, 192), (32, 96), 64) \n",
        "        self.maxpool3 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2,\n",
        "                                                  padding='same')\n",
        "        \n",
        "        self.inception4a = InceptionBlock(192, (96, 208), (16, 48), 64) \n",
        "        self.inception4b = InceptionBlock(160, (112, 224), (24, 64), 64) \n",
        "        self.inception4c = InceptionBlock(128, (128, 256), (24, 64), 64) \n",
        "        self.inception4d = InceptionBlock(112, (144, 288), (32, 64), 64) \n",
        "        self.inception4e = InceptionBlock(256, (160, 320), (32, 128), 128)\n",
        "        self.maxpool4 = tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2, \n",
        "                                                  padding='same')\n",
        "        \n",
        "        self.inception5a = InceptionBlock(256, (160, 320), (32, 128), 128)\n",
        "        self.inception5b = InceptionBlock(384, (192, 384), (48, 128), 128) \n",
        "        self.avgpool = tf.keras.layers.AveragePooling2D(pool_size=(7, 7), strides=1)\n",
        "        \n",
        "        self.dropout = tf.keras.layers.Dropout(rate=0.4)\n",
        "        self.dense = tf.keras.layers.Dense(units=num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, X):\n",
        "        X = self.conv1(X)\n",
        "        X = self.maxpool1(X)\n",
        "        X = self.conv2(X)\n",
        "        X = self.maxpool2(X)\n",
        "        X = self.inception3a(X)\n",
        "        X = self.inception3b(X)\n",
        "        X = self.maxpool3(X)\n",
        "        X = self.inception4a(X)\n",
        "        X = self.inception4b(X)\n",
        "        X = self.inception4c(X)\n",
        "        X = self.inception4d(X)\n",
        "        X = self.inception4e(X)\n",
        "        X = self.maxpool4(X)\n",
        "        X = self.inception5a(X)\n",
        "        X = self.inception5b(X)\n",
        "        X = self.avgpool(X)\n",
        "        X = self.dropout(X)\n",
        "        return self.dense(X)\n",
        "    "
      ],
      "metadata": {
        "id": "FCmjSJklyumv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionNet(num_classes=1000)\n",
        "X = tf.random.uniform((1, 224, 224, 3))\n",
        "model(X)"
      ],
      "metadata": {
        "id": "lU9oGMei5k3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c142774-a221-456f-fd49-4bae95fb6433"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 1, 1000), dtype=float32, numpy=\n",
              "array([[[[0.00099327, 0.00098925, 0.0009883 , 0.00100536, 0.00099733,\n",
              "          0.00098871, 0.00100068, 0.0009975 , 0.00100316, 0.00098823,\n",
              "          0.0010083 , 0.00099416, 0.00100492, 0.00101271, 0.00099818,\n",
              "          0.00100651, 0.00099659, 0.00099071, 0.00099774, 0.00099708,\n",
              "          0.00099664, 0.00100016, 0.00099657, 0.00100114, 0.00099016,\n",
              "          0.00099457, 0.00099508, 0.0009994 , 0.00100112, 0.00100383,\n",
              "          0.00099607, 0.00099237, 0.00099639, 0.00101102, 0.00099551,\n",
              "          0.00099269, 0.00099559, 0.00099223, 0.00100049, 0.00100251,\n",
              "          0.00099825, 0.00099882, 0.00099319, 0.0009999 , 0.00099652,\n",
              "          0.00100231, 0.00098861, 0.00098654, 0.00100349, 0.00100013,\n",
              "          0.00100634, 0.00101662, 0.00100875, 0.00100407, 0.000995  ,\n",
              "          0.00099598, 0.00099911, 0.00099602, 0.00100599, 0.00098786,\n",
              "          0.001001  , 0.00099766, 0.00099572, 0.00100327, 0.001003  ,\n",
              "          0.00100224, 0.00100246, 0.00099732, 0.00099789, 0.00099259,\n",
              "          0.00099278, 0.00100974, 0.00099301, 0.00099315, 0.00100539,\n",
              "          0.00099713, 0.00100899, 0.00099959, 0.00099752, 0.00099893,\n",
              "          0.00099071, 0.00100205, 0.0010109 , 0.00099776, 0.00098779,\n",
              "          0.00100529, 0.00099331, 0.00100338, 0.00099907, 0.00100378,\n",
              "          0.00099888, 0.00099861, 0.00099433, 0.00100014, 0.00100692,\n",
              "          0.00100878, 0.00099905, 0.00099811, 0.00099654, 0.00101047,\n",
              "          0.00100452, 0.00099765, 0.00099092, 0.00100019, 0.00100011,\n",
              "          0.00100293, 0.00100378, 0.00100796, 0.00099787, 0.0010101 ,\n",
              "          0.00100752, 0.00100004, 0.00100191, 0.0009899 , 0.0010003 ,\n",
              "          0.00098821, 0.00100363, 0.00100354, 0.00100968, 0.00099803,\n",
              "          0.0010008 , 0.00098411, 0.00099412, 0.0010094 , 0.00099808,\n",
              "          0.00101029, 0.00099917, 0.00099054, 0.00100977, 0.00099297,\n",
              "          0.00100269, 0.00100219, 0.00100802, 0.00100205, 0.00099662,\n",
              "          0.00099099, 0.00099134, 0.00100696, 0.00099412, 0.00099464,\n",
              "          0.00100134, 0.0010003 , 0.00100973, 0.00100834, 0.00100308,\n",
              "          0.00099078, 0.00101716, 0.00098961, 0.000995  , 0.00099485,\n",
              "          0.00100214, 0.00100195, 0.00099325, 0.00100181, 0.00100818,\n",
              "          0.00100738, 0.00099403, 0.0010027 , 0.00099401, 0.00101003,\n",
              "          0.00100438, 0.00099403, 0.00100602, 0.00099468, 0.00100317,\n",
              "          0.00100341, 0.00100081, 0.00099324, 0.00100145, 0.0009937 ,\n",
              "          0.00100423, 0.00099365, 0.00099457, 0.00100093, 0.00099589,\n",
              "          0.00099232, 0.00100576, 0.0009942 , 0.00099787, 0.00100113,\n",
              "          0.00099124, 0.00100699, 0.00099881, 0.00101075, 0.00099214,\n",
              "          0.00100056, 0.00101195, 0.00100033, 0.00099505, 0.00099637,\n",
              "          0.00100437, 0.00101199, 0.00100426, 0.00100485, 0.00099489,\n",
              "          0.00100308, 0.00098176, 0.00100813, 0.00100923, 0.0009928 ,\n",
              "          0.00099529, 0.00098982, 0.00100841, 0.00100076, 0.00099265,\n",
              "          0.00101032, 0.00099333, 0.00100386, 0.00099799, 0.00100086,\n",
              "          0.00100728, 0.00100643, 0.00100901, 0.00099022, 0.00101085,\n",
              "          0.00100471, 0.0010122 , 0.0010015 , 0.00099952, 0.00099991,\n",
              "          0.00099761, 0.00099856, 0.00101543, 0.00100107, 0.00099188,\n",
              "          0.00099886, 0.00099662, 0.00100701, 0.0009961 , 0.0009996 ,\n",
              "          0.00100338, 0.00099728, 0.00100651, 0.00099464, 0.00100139,\n",
              "          0.00099523, 0.0009993 , 0.00098942, 0.00099643, 0.00100037,\n",
              "          0.00100417, 0.00099762, 0.00100831, 0.00099924, 0.00100357,\n",
              "          0.00100224, 0.00100263, 0.00099468, 0.00100205, 0.00099768,\n",
              "          0.00100744, 0.00099222, 0.00099158, 0.00099155, 0.00100368,\n",
              "          0.00099688, 0.0009956 , 0.00098703, 0.00099536, 0.00099419,\n",
              "          0.00100456, 0.00099385, 0.000995  , 0.00099915, 0.00100254,\n",
              "          0.00099784, 0.00098761, 0.00100763, 0.00100244, 0.00099437,\n",
              "          0.00100262, 0.00099176, 0.00100335, 0.00101139, 0.00099447,\n",
              "          0.00101527, 0.00099076, 0.00099667, 0.0010026 , 0.00100732,\n",
              "          0.00099766, 0.00100309, 0.0009992 , 0.00099452, 0.00100213,\n",
              "          0.00100357, 0.00100727, 0.00099723, 0.00099797, 0.00100763,\n",
              "          0.00099587, 0.00100593, 0.00099936, 0.00101087, 0.0009922 ,\n",
              "          0.00099426, 0.00099815, 0.00100362, 0.00099949, 0.00099657,\n",
              "          0.00100258, 0.00100757, 0.00100373, 0.00098526, 0.00097979,\n",
              "          0.00099876, 0.00100157, 0.00100769, 0.00099516, 0.00099996,\n",
              "          0.00100655, 0.00099504, 0.00100349, 0.00100643, 0.00100432,\n",
              "          0.00099858, 0.00100301, 0.00100461, 0.00100312, 0.00099375,\n",
              "          0.00099415, 0.00099361, 0.00100155, 0.00099563, 0.0009918 ,\n",
              "          0.00099758, 0.00099589, 0.00100505, 0.00100419, 0.00099575,\n",
              "          0.00100131, 0.0009989 , 0.00100621, 0.00100417, 0.00100267,\n",
              "          0.00099032, 0.00100513, 0.00098854, 0.00099614, 0.00100469,\n",
              "          0.00101024, 0.00100303, 0.00099942, 0.00099778, 0.00099824,\n",
              "          0.00100234, 0.00099122, 0.00099645, 0.00099842, 0.00100562,\n",
              "          0.00099116, 0.00100403, 0.00099772, 0.00099976, 0.0009972 ,\n",
              "          0.00099633, 0.00100591, 0.00099225, 0.00099485, 0.0009999 ,\n",
              "          0.00099698, 0.00100279, 0.00100918, 0.00100334, 0.00099749,\n",
              "          0.00100749, 0.00098903, 0.00099864, 0.00100442, 0.00101007,\n",
              "          0.00099894, 0.0010068 , 0.00100472, 0.00099451, 0.00101082,\n",
              "          0.00099717, 0.00100522, 0.00099766, 0.00099909, 0.00099631,\n",
              "          0.00099808, 0.0009979 , 0.00099363, 0.00100552, 0.00101432,\n",
              "          0.00100756, 0.00099019, 0.00099637, 0.00100079, 0.00100478,\n",
              "          0.00100728, 0.00099864, 0.00098948, 0.00099775, 0.00100523,\n",
              "          0.00100771, 0.00099851, 0.0009922 , 0.000997  , 0.001003  ,\n",
              "          0.00099705, 0.00099904, 0.00101075, 0.00099459, 0.00100919,\n",
              "          0.00098796, 0.0009763 , 0.00100256, 0.00100104, 0.00100141,\n",
              "          0.00099854, 0.00100357, 0.00100641, 0.00100021, 0.00099779,\n",
              "          0.0010097 , 0.0010084 , 0.0010048 , 0.00101726, 0.00099209,\n",
              "          0.00100877, 0.00101603, 0.00099323, 0.00099761, 0.00099363,\n",
              "          0.00100243, 0.00099454, 0.00100982, 0.00100123, 0.00100579,\n",
              "          0.00100588, 0.00099354, 0.00099635, 0.00099802, 0.00100217,\n",
              "          0.00099453, 0.00100105, 0.00100081, 0.00100331, 0.00099059,\n",
              "          0.00100153, 0.0010134 , 0.00100557, 0.00098911, 0.00099929,\n",
              "          0.00099214, 0.0009974 , 0.00099613, 0.00099945, 0.00100162,\n",
              "          0.00099978, 0.00100013, 0.00099927, 0.00100697, 0.00098924,\n",
              "          0.00100445, 0.00099282, 0.00100106, 0.0009942 , 0.00101128,\n",
              "          0.00098697, 0.00100875, 0.00100559, 0.00099562, 0.00101009,\n",
              "          0.00100411, 0.00100423, 0.00098801, 0.000998  , 0.00101821,\n",
              "          0.00099544, 0.00100275, 0.00098842, 0.00100297, 0.00098701,\n",
              "          0.0009977 , 0.001006  , 0.00099816, 0.00099884, 0.00100349,\n",
              "          0.00100236, 0.00100515, 0.00100116, 0.00099193, 0.00100496,\n",
              "          0.0009959 , 0.00100953, 0.00099617, 0.00099715, 0.00099374,\n",
              "          0.00099443, 0.0010024 , 0.00099236, 0.00101254, 0.00100032,\n",
              "          0.00099806, 0.00100021, 0.00098468, 0.00100324, 0.00100479,\n",
              "          0.0010093 , 0.00101668, 0.00099579, 0.00100065, 0.0010049 ,\n",
              "          0.00100304, 0.00101583, 0.00100533, 0.00099796, 0.00098556,\n",
              "          0.00100295, 0.00100213, 0.0010086 , 0.00099551, 0.00100043,\n",
              "          0.00100578, 0.00099426, 0.0009907 , 0.0009949 , 0.00099927,\n",
              "          0.0010015 , 0.00100028, 0.00099866, 0.0009944 , 0.00100555,\n",
              "          0.00098427, 0.00099829, 0.00099328, 0.00101071, 0.00098706,\n",
              "          0.00099534, 0.00100998, 0.00100083, 0.00100623, 0.00099112,\n",
              "          0.00099947, 0.00100152, 0.00100975, 0.00100682, 0.0010039 ,\n",
              "          0.00099188, 0.00100704, 0.0010171 , 0.00100324, 0.0010008 ,\n",
              "          0.00100295, 0.00099657, 0.00099946, 0.00099502, 0.00100673,\n",
              "          0.00100503, 0.00100076, 0.00099735, 0.00100103, 0.00099333,\n",
              "          0.00100262, 0.00099918, 0.00100995, 0.00099538, 0.0010041 ,\n",
              "          0.00100528, 0.00099628, 0.00099115, 0.00100052, 0.00099972,\n",
              "          0.00099956, 0.00099773, 0.00100209, 0.00100135, 0.00099239,\n",
              "          0.00099994, 0.00100884, 0.00098903, 0.00099384, 0.00101132,\n",
              "          0.00100786, 0.0010166 , 0.00099826, 0.00100399, 0.00098588,\n",
              "          0.00101269, 0.00099115, 0.00099162, 0.00100136, 0.00100819,\n",
              "          0.00100094, 0.00100194, 0.00100463, 0.00099621, 0.00099492,\n",
              "          0.00099078, 0.00100754, 0.00099542, 0.00100746, 0.00099345,\n",
              "          0.0010006 , 0.00099634, 0.00100124, 0.0009943 , 0.00098975,\n",
              "          0.00100643, 0.00100455, 0.0009968 , 0.00100029, 0.00100204,\n",
              "          0.00099017, 0.00100541, 0.00099627, 0.00099781, 0.00100478,\n",
              "          0.00099729, 0.0010134 , 0.0009927 , 0.00099618, 0.00098932,\n",
              "          0.00100374, 0.00099069, 0.00099409, 0.0009948 , 0.00098602,\n",
              "          0.0010066 , 0.00100705, 0.00099626, 0.00100802, 0.00101011,\n",
              "          0.00100498, 0.00100331, 0.00099837, 0.00099458, 0.00099974,\n",
              "          0.00099934, 0.00100711, 0.00100002, 0.00099821, 0.00100198,\n",
              "          0.0009952 , 0.00100419, 0.00099899, 0.00100167, 0.00099372,\n",
              "          0.00099818, 0.00099358, 0.00101463, 0.00099988, 0.00100087,\n",
              "          0.00099437, 0.00100019, 0.00100203, 0.00101092, 0.00098789,\n",
              "          0.00100725, 0.00100327, 0.00100155, 0.00099081, 0.00100823,\n",
              "          0.00100633, 0.0010034 , 0.00100236, 0.00100041, 0.00099907,\n",
              "          0.00100165, 0.00100491, 0.00099511, 0.00101076, 0.00100201,\n",
              "          0.00100574, 0.00099765, 0.00100037, 0.00101513, 0.00099668,\n",
              "          0.0009942 , 0.00100972, 0.00099791, 0.00099163, 0.00099466,\n",
              "          0.00100227, 0.00099785, 0.00100238, 0.0009994 , 0.00100081,\n",
              "          0.00099969, 0.00100352, 0.00100241, 0.00099746, 0.00100882,\n",
              "          0.00100008, 0.00098518, 0.00099821, 0.00100008, 0.00099896,\n",
              "          0.00100664, 0.00099771, 0.00098173, 0.00100549, 0.00100394,\n",
              "          0.00100015, 0.00098484, 0.00100738, 0.00100691, 0.00099312,\n",
              "          0.00100642, 0.00101413, 0.00099965, 0.00099768, 0.00098704,\n",
              "          0.00100049, 0.00099517, 0.00098711, 0.00099883, 0.001013  ,\n",
              "          0.00098967, 0.00099838, 0.00099642, 0.00101032, 0.00099255,\n",
              "          0.0010091 , 0.00100101, 0.00099901, 0.00100561, 0.00100851,\n",
              "          0.00100848, 0.00100554, 0.00098836, 0.00099873, 0.00101523,\n",
              "          0.00100057, 0.00098983, 0.00100364, 0.0009943 , 0.0009961 ,\n",
              "          0.00100145, 0.00100779, 0.00099495, 0.00099438, 0.00099488,\n",
              "          0.00099509, 0.00098463, 0.00101472, 0.00100291, 0.00099913,\n",
              "          0.00100543, 0.00101306, 0.00099071, 0.00099293, 0.00100602,\n",
              "          0.00100822, 0.00100016, 0.00101003, 0.00099711, 0.00098762,\n",
              "          0.00100572, 0.00099727, 0.00100786, 0.00098799, 0.00099899,\n",
              "          0.00100346, 0.00100313, 0.00100716, 0.00099148, 0.00101008,\n",
              "          0.00100319, 0.00099866, 0.00100244, 0.00099661, 0.00099396,\n",
              "          0.00100973, 0.00100854, 0.00099984, 0.00099467, 0.00100183,\n",
              "          0.00099932, 0.00100867, 0.00098579, 0.00100349, 0.00099769,\n",
              "          0.00100661, 0.00100643, 0.00100573, 0.00100639, 0.00099408,\n",
              "          0.0010071 , 0.00100303, 0.00099395, 0.00100979, 0.00098881,\n",
              "          0.00099215, 0.00099565, 0.00099055, 0.00099752, 0.00100099,\n",
              "          0.00100243, 0.00099287, 0.00098802, 0.00099413, 0.00100338,\n",
              "          0.00100769, 0.00099703, 0.00099887, 0.00099683, 0.00100434,\n",
              "          0.00101685, 0.00099388, 0.00099372, 0.0010008 , 0.00099139,\n",
              "          0.00099821, 0.00099448, 0.00100108, 0.00099375, 0.00099925,\n",
              "          0.00099827, 0.00100056, 0.00099802, 0.00098729, 0.00099256,\n",
              "          0.00100991, 0.00099686, 0.00099566, 0.00100086, 0.00100827,\n",
              "          0.00100222, 0.00100474, 0.00100167, 0.00100712, 0.00099423,\n",
              "          0.00100944, 0.00100987, 0.00099903, 0.00099205, 0.00098744,\n",
              "          0.0009917 , 0.00098148, 0.00099334, 0.00098264, 0.00101052,\n",
              "          0.00099446, 0.0010024 , 0.00099343, 0.00099885, 0.00100428,\n",
              "          0.00100819, 0.00099499, 0.00099984, 0.00099451, 0.0010119 ,\n",
              "          0.00099722, 0.0010019 , 0.00099916, 0.00100872, 0.00099552,\n",
              "          0.00100346, 0.00101204, 0.00099825, 0.00099555, 0.00100014,\n",
              "          0.00099353, 0.00100996, 0.00100017, 0.0010159 , 0.00099117,\n",
              "          0.00101391, 0.00098841, 0.00099819, 0.00100015, 0.00100614,\n",
              "          0.00099093, 0.00100129, 0.00099868, 0.00099427, 0.00101401,\n",
              "          0.00100189, 0.00100148, 0.00100928, 0.00100358, 0.00099882,\n",
              "          0.00100333, 0.00100176, 0.00100834, 0.00099866, 0.0009949 ,\n",
              "          0.00099159, 0.00100402, 0.0009932 , 0.00099662, 0.00099961,\n",
              "          0.00100362, 0.00101162, 0.00100301, 0.00100537, 0.00101955,\n",
              "          0.00097898, 0.00099387, 0.00099226, 0.00099011, 0.00099097,\n",
              "          0.00100405, 0.00100178, 0.00100644, 0.00100823, 0.00098734,\n",
              "          0.00100313, 0.00099643, 0.0009931 , 0.00099599, 0.00100095,\n",
              "          0.0010057 , 0.00101218, 0.00100066, 0.00100429, 0.00100084,\n",
              "          0.00099525, 0.00099431, 0.00101344, 0.00100269, 0.00100006,\n",
              "          0.00100653, 0.0009893 , 0.00100977, 0.00099349, 0.00100392,\n",
              "          0.00100247, 0.00100931, 0.00099651, 0.00098549, 0.00099217,\n",
              "          0.00100412, 0.00100069, 0.00099673, 0.00100398, 0.00100326,\n",
              "          0.00100718, 0.00100036, 0.00099707, 0.00100135, 0.00100383,\n",
              "          0.00100337, 0.00099359, 0.0009928 , 0.0010005 , 0.00099079,\n",
              "          0.0009927 , 0.00100208, 0.00100882, 0.00100435, 0.00100962,\n",
              "          0.00101485, 0.0010017 , 0.00100218, 0.00099029, 0.00098903,\n",
              "          0.00100235, 0.00100451, 0.00100697, 0.00099967, 0.0010006 ,\n",
              "          0.00100529, 0.00101178, 0.00099613, 0.00099498, 0.00100473,\n",
              "          0.00100378, 0.00099767, 0.00101115, 0.00100087, 0.00099341,\n",
              "          0.00098649, 0.00100045, 0.00100348, 0.00099354, 0.00100123,\n",
              "          0.00100582, 0.00098616, 0.00100588, 0.00099253, 0.00101081,\n",
              "          0.00100112, 0.00099245, 0.00099666, 0.0010064 , 0.00101087,\n",
              "          0.00100192, 0.00099625, 0.00100728, 0.00099389, 0.00099251,\n",
              "          0.00098812, 0.00099239, 0.00099814, 0.00100541, 0.00100956,\n",
              "          0.00099808, 0.00100827, 0.00098925, 0.00100167, 0.00100911,\n",
              "          0.00100821, 0.00099116, 0.00100084, 0.00099867, 0.00099502]]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "FTR9Uy_g5lXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bd634e-7331-4b09-ceda-3d04c522b53f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"inception_net_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_119 (Conv2D)         multiple                  9472      \n",
            "                                                                 \n",
            " max_pooling2d_27 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_120 (Conv2D)         multiple                  110784    \n",
            "                                                                 \n",
            " max_pooling2d_28 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " inception_block_19 (Incepti  multiple                 163696    \n",
            " onBlock)                                                        \n",
            "                                                                 \n",
            " inception_block_20 (Incepti  multiple                 388736    \n",
            " onBlock)                                                        \n",
            "                                                                 \n",
            " max_pooling2d_31 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " inception_block_21 (Incepti  multiple                 376176    \n",
            " onBlock)                                                        \n",
            "                                                                 \n",
            " inception_block_22 (Incepti  multiple                 449160    \n",
            " onBlock)                                                        \n",
            "                                                                 \n",
            " inception_block_23 (Incepti  multiple                 510104    \n",
            " onBlock)                                                        \n",
            "                                                                 \n",
            " inception_block_24 (Incepti  multiple                 605376    \n",
            " onBlock)                                                        \n",
            "                                                                 \n",
            " inception_block_25 (Incepti  multiple                 868352    \n",
            " onBlock)                                                        \n",
            "                                                                 \n",
            " max_pooling2d_37 (MaxPoolin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " inception_block_26 (Incepti  multiple                 1043456   \n",
            " onBlock)                                                        \n",
            "                                                                 \n",
            " inception_block_27 (Incepti  multiple                 1444080   \n",
            " onBlock)                                                        \n",
            "                                                                 \n",
            " average_pooling2d_2 (Averag  multiple                 0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         multiple                  0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  1025000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,994,392\n",
            "Trainable params: 6,994,392\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and pre-processing dataset"
      ],
      "metadata": {
        "id": "7nyUNs2hDsIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1RL0T7Rg4XqQNRCkjfnLo4goOJQ7XZro9\n",
        "!unzip cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "id": "lJgAXBlVDNna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40, \n",
        "                                   width_shift_range=0.2, height_shift_range=0.2,\n",
        "                                   shear_range=0.2, zoom_range=0.2, \n",
        "                                   horizontal_flip=True, vertical_flip=True, \n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "train_dataset = train_datagen.flow_from_directory(directory='/content/cats_and_dogs_filtered/train', batch_size=20,\n",
        "                                                  class_mode='categorical', target_size=(224, 224))\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_dataset = val_datagen.flow_from_directory(directory='/content/cats_and_dogs_filtered/validation', batch_size=20, \n",
        "                                              class_mode='categorical', target_size=(224, 224))"
      ],
      "metadata": {
        "id": "ERlJV9Y-Dxdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionNet(num_classes=2)\n",
        "model.build(input_shape=(None, 224, 224, 3))"
      ],
      "metadata": {
        "id": "1jxT1IsLEjYP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "batch_size = 20\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_function(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits) \n",
        "    return loss_value \n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train) \n",
        "        if step % 100 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))  \n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc at step: %.4f\" % (float(train_acc),))\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))                         "
      ],
      "metadata": {
        "id": "alGj5Kv2D-hc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}